---
title: "Homework 5 solutions"
author: "Rio Yan"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 9,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1

__Describe the raw data:__
The data included the location of the killing, whether an arrest was made and, in most cases, basic demographic information about each victim in 50 large U.S. cities. There was one data entered error, Tulsa_AL, and got cleaned out during the cleaning process. 

Read in the data.
```{r}
homicide_df = 
  read_csv("homicide_data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved"
    )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL")
```


Let's look at this a bit

```{r}
aggregate_df = 
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )
```

Can I do a prop test for a single city?

```{r}
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved),
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)
) %>% 
  broom::tidy()
```

Try to iterate ...

```{r}
results_df = 
  aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```


```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

## Problem 2

import one dataset 

```{r}
data_1 = read_csv("lda_data/con_01.csv")
```

```{r, warning = FALSE, message = FALSE}
path_df = 
  tibble(
    path = list.files("lda_data"),
  ) %>% 
  mutate(
    path = str_c("lda_data/", path),
    data = map(.x = path, ~read_csv(.x)),
    path = str_remove(path, ".csv")) %>% 
  separate(path, into = c("cut", "path"), sep = "/") %>%  
  separate(path, into = c("control_arm", "subject_id"), sep = c("_")) %>% 
  select(-cut) %>% 
  unnest(data)
```

plot
```{r}
path_df_graph =
  path_df %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week_num",
    values_to = "value"
  ) %>% 
  mutate(
    control_arm = recode_factor(control_arm,"con" = "control", "exp" = "experiment")
  )

path_df_graph %>% 
  ggplot(aes(x = week_num, y = value, 
             group = subject_id, color = factor(control_arm))) +
  geom_line() + geom_point() +
  facet_grid(. ~ control_arm) +
  labs(title = "Observations on each subject between two groups",
       x = "Week Number",
       y = "Value") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

__Comment on the differences between groups:__

The graph shows that the experiment group's cluster is higher than the control one, indicating that the experimental group overall has higher values than control group. In addition, the experiment group seems to have a positive association between week counts and values, as the week progress, the higher the value the subjects have. However, the control group doesn't seem to have an association between week number and their value. 


## Problem 3

```{r}
set.seed(1)

sim_mean_sd = function(n, mu, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n = 30, mean = mu, sd = sigma)
  ) 
  
  sim_data %>% 
    t.test() %>% 
    broom::tidy() %>% 
    select(estimate, p.value)
  
}

sim_results = 
  tibble(mu_list = c(0, 1, 2, 3, 4, 5, 6)) %>% 
  mutate(
    output_lists = map(.x = mu_list, ~rerun(5000, sim_mean_sd(n = 30, mu = .x))), 
    estimate_dfs = map(output_lists, bind_rows)) %>%
    unnest(estimate_dfs) %>% 
  select(-output_lists)
```


Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. 
```{r}
reject_table = 
  sim_results %>% 
  mutate(
    decision = 
      case_when(
        p.value >= 0.05 ~ "fail to reject",
        p.value < 0.05 ~ "reject"
      )) 

reject_prop = 
  reject_table %>% 
  group_by(decision, mu_list) %>% 
  summarize(
    count = n()
  ) %>% 
  filter(decision == "reject") %>% 
  mutate(
    prop_reject = count / 5000)

prop_data1 = 
  left_join(sim_results, reject_prop, by = "mu_list")
  

prop_data1 %>% 
  ggplot(aes(x = mu_list, y = prop_reject, color = decision)) +
  geom_point() + geom_line() +
  labs(title = "Association between effect size and power",
       x = " true value of μ",
       y = "power of the test") +
  guides(color = guide_legend(title = "Prop of rejection")) 
```

__Describe the association between effect size and power:__
The graph shows that as the true value of mu increases, the power of the test increases and eventfully reaches 1, meaning there is a positive association between true value of mu and the power of the test. However, when the true value is the same as the null(0), there is still some chance that the null will be rejected, this is due to randomness.


```{r}
mean_table = 
  sim_results %>% 
  group_by(mu_list) %>% 
  summarize(mean_mu = mean(estimate))

#plot showing average estimate of mu on the y axis and the true value of mu on x axis
plot_1 = 
  mean_table %>% 
  ggplot(aes(x = mu_list, y = mean_mu)) + 
  geom_point() + geom_line() +
  labs(title = "Association between true value of mu and average estimate of mu",
       x = " true value of μ",
       y = "Average estimate of mu")

reject_mean = 
  reject_table %>% 
  filter(decision == "reject") %>% 
  group_by(mu_list) %>% 
  summarize(
    mean_estimate = mean(estimate)
  )

#plot second plot the average estimate in samples for which the null was rejected on y and the true value mu on x 
plot_2 =
  reject_mean %>% 
  ggplot(aes(x = mu_list, y = mean_estimate)) +
  geom_point() + geom_line() +
  labs(title = "Association between true value of mu and rejected average estimate of mu",
       x = " true value of μ",
       y = "rejected mean estimate of mu")

  
# combined plot 
combine_plot = 
  left_join(mean_table, reject_mean, by = "mu_list") %>%  
  pivot_longer(
    mean_mu:mean_estimate,
    names_to = "type",
    values_to = "average_estimated_mu"
  ) %>% 
  ggplot(aes(x = mu_list, y = average_estimated_mu, color = type))+
  geom_point() + geom_line() +
  labs(title = "Comparison of average estimated mu between rejected average and true mu ",
       x = " true value of μ",
       y = "average estimated mu") +
  guides(color = guide_legend(title = "Type of average")) 
  
combine_plot
```

__Comment on the overlaid plot:__
From the graph, we can see that the sample average of mu across tests for which the null is rejected approximates the true value of mu as the true value of mu gets bigger. In this graph, the sample average of mu approximately equal to the true mean when the true mean is greater than 3, but is greater than the true mean when the true mean is less than or equal to 2. 

This is because when we pick samples for which the null is rejected, we were picking samples that are far away from the null(0), so the random samples from normal distribution with true mean farther from 0 were selected more and the mean of sample means approximates the corresponding true mean. One explanation for the apparent overestimated estimated mu when true mu = 2 could be by chance we were getting high numbers. Naturally, when true effect size is small, some chance that the average estimate will be higher than true value.



